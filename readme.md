# Tesis ‚Äî Data Lake + EDA

Este repositorio contiene el **setup de datos** (GCS + Parquet) y el **EDA** inicial para la tesis _‚ÄúTecnolog√≠as digitales y su impacto en la inercia del comportamiento de viaje en transporte p√∫blico‚Äù_ (DCC + MDS, U. de Chile).

El proyecto trabaja **100% contra Google Cloud Storage (GCS)**: los CSV viven en `gs://` y se convierten a **Parquet particionado** para an√°lisis r√°pidos y reproducibles.

## Arquitectura del proyecto

```
tesis-project/
‚îú‚îÄ 00_setup/
‚îÇ  ‚îî‚îÄ setup.qmd                # Validaci√≥n de la capa Bronze
‚îú‚îÄ 01_processing/
‚îÇ  ‚îî‚îÄ 01_silver_processing.qmd # Procesamiento Bronze‚ÜíSilver
‚îú‚îÄ 02_eda/
‚îÇ  ‚îú‚îÄ eda_caracterizacion.qmd  # Perfil demogr√°fico de usuarios QR
‚îÇ  ‚îú‚îÄ eda_trips_overview.qmd   # An√°lisis de patrones de viaje
‚îÇ  ‚îî‚îÄ join_validation.qmd      # Validaci√≥n de joins
‚îú‚îÄ lib/
‚îÇ  ‚îî‚îÄ datalake.py              # Helpers (enable_adc, scan_parquet_portable)
‚îú‚îÄ process_data.py             # Pipeline de ingesta RAW‚ÜíBronze
‚îú‚îÄ _quarto.yml                 # Config del sitio Quarto
‚îú‚îÄ GIT_WORKFLOW.md             # Gu√≠a de workflow de Git
‚îú‚îÄ _site/                      # (generado) HTML renderizados
‚îî‚îÄ .quarto/                    # (generado) cach√©s de Quarto
```

> Las carpetas **`_site/`**, **`_freeze/`** y **`.quarto/`** son artefactos generados por Quarto y **no** deben versionarse.

### Data Lake en GCS

- `gs://tesis-vonetto-datalake/raw/`  
  CSV originales (solo lectura).

- `gs://tesis-vonetto-datalake/lake/bronze/`  
  **Bronze** = datos "aterrizados" en Parquet, con cambios m√≠nimos (parseo de fechas y particiones).
  - `bronze/viajes/semana_iso=YYYY-WNN/part-*.parquet` (particionado por semana ISO)
  - `bronze/caracterizacion/snapshot_date=YYYY-MM-DD/part-*.parquet` (snapshot √∫nico)

- `gs://tesis-vonetto-datalake/lake/silver/`  
  **Silver** = limpieza y enriquecimiento reutilizable (nombres normalizados, tipos, dedupe, features b√°sicos).

- `gs://tesis-vonetto-datalake/lake/gold/`  
  **Gold** = datasets curados para preguntas/outputs concretos (KPIs, paneles por usuario-semana, m√©tricas de inercia, matrices OD).

Resumen:
- **Bronze**: formato anal√≠tico + partici√≥n, casi sin ‚Äúcocina‚Äù.
- **Silver**: datos **consistentes** y **reutilizables** para EDA/joins.
- **Gold**: tablas **listas para el paper/modelos**.

## Requisitos

- **Python 3.10+** (se prob√≥ con Python 3.13 y Polars ‚â• 1.33, PyArrow ‚â• 11)
- **Quarto ‚â• 1.4** (`quarto --version`)
- **Google Cloud SDK** (`gcloud`) para autenticaci√≥n

### Paquetes Python

**Core (obligatorios):**
- `polars` - Procesamiento de datos r√°pido
- `pyarrow` - Backend de Parquet
- `gcsfs` - Acceso a Google Cloud Storage
- `fsspec` - Sistema de archivos abstracto

**An√°lisis y visualizaci√≥n:**
- `matplotlib` - Gr√°ficos
- `seaborn` - Visualizaciones estad√≠sticas
- `geopandas` - Datos geoespaciales
- `pyogrio` - Lectura eficiente de shapefiles

**Utilidades:**
- `tqdm` - Progress bars

**Instalaci√≥n completa:**
```bash
pip install polars pyarrow gcsfs fsspec matplotlib seaborn geopandas pyogrio tqdm
```

## Autenticaci√≥n (GCP)

Una vez por equipo:
```bash
gcloud auth application-default login
````

O define `GOOGLE_APPLICATION_CREDENTIALS` apuntando a un JSON de Service Account.
Los notebooks usan un helper (`enable_adc`) que levanta autom√°ticamente las credenciales.

## Configuraci√≥n r√°pida

### 1. Crear entorno virtual e instalar dependencias

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -U polars pyarrow gcsfs fsspec matplotlib seaborn geopandas pyogrio tqdm
```

### 2. Autenticaci√≥n con GCP

```bash
gcloud auth application-default login
```

### 3. Pipeline de Datos (ejecutar en orden)

#### a) Ingesta RAW ‚Üí Bronze (primera vez o al agregar datos nuevos)

```bash
python process_data.py
```

Esto lee los CSVs de `gs://tesis-vonetto-datalake/raw/` y los escribe como Parquet particionado en `bronze/viajes/`.

**‚è±Ô∏è Tiempo:** Puede tomar varios minutos dependiendo del volumen de datos.

#### b) Validaci√≥n de Bronze (opcional pero recomendado)

```bash
quarto render 00_setup/setup.qmd
```

Valida que las particiones se escribieron correctamente y genera un reporte HTML.

#### c) Procesamiento Silver (cuando necesites datos enriquecidos)

```bash
quarto render 01_processing/01_silver_processing.qmd
```

Aplica enriquecimiento geogr√°fico y crea primary keys. **Nota:** El join geogr√°fico est√° pendiente de completarse.

#### d) An√°lisis Exploratorio (EDA)

```bash
# An√°lisis demogr√°fico de usuarios QR
quarto render 02_eda/eda_caracterizacion.qmd

# An√°lisis de patrones de viaje (parametrizable)
quarto render 02_eda/eda_trips_overview.qmd

# O en modo preview interactivo:
quarto preview 02_eda/eda_caracterizacion.qmd
```

Los HTMLs generados quedan en `_site/` y en las carpetas `*_files/`.

## Par√°metros y variables de entorno

* `EDA_YEAR` / `EDA_WEEK` (int): seleccionan particiones.
* `EDA_SAVE_GOLD` (bool-like): si el EDA exporta productos.
* `GOOGLE_APPLICATION_CREDENTIALS`: ruta a credenciales (opcional si usas ADC).
* `QUARTO_PROJECT_DIR`: resuelve rutas a `lib/` cuando ejecutas fuera de la ra√≠z.

## Buenas pr√°cticas

* **Bronze es idempotente**: pol√≠ticas `write_missing`/`upsert` evitan re-escrituras completas.
* **Silver/Gold** deber√°n incluir **checks** (conteos, nulos, rangos) y **metadatos** (fecha, versi√≥n de c√≥digo).
* **Privacidad**: todo est√° pseudonimizado; no subir credenciales ni datos sensibles locales al repo.

## Workflow de Git

Este proyecto usa una estrategia de ramas estructurada:
- `main` - C√≥digo estable para hitos importantes
- `develop` - Desarrollo activo y trabajo diario
- `feature/*` - Funcionalidades nuevas
- `docs/*` - Documentaci√≥n y paper

üìñ **Ver gu√≠a completa en:** [`GIT_WORKFLOW.md`](GIT_WORKFLOW.md)

## Contribuciones

Este es un proyecto de tesis. Para dudas o colaboraciones, contactar a Juan Vicente Onetto Romero.

## Licencia

Por definir



