---
title: "Tesis — 00 | Validación del Data Lake (Bronze)"
author: "Juan Vicente Onetto Romero"
format:
  html:
    toc: true
    code-fold: true
jupyter: python3
freeze: false
---

# Objetivo y Flujo de Trabajo

Este notebook valida la integridad de la capa **Bronze** de nuestro Data Lake en GCS.

El flujo de trabajo es:

1.  **Procesar Datos:** Para ingestar nuevos archivos, ejecuta `python process_data.py` en la terminal.
2.  **Validar en este Notebook:** Ejecuta las celdas de este notebook para:
    *   Ver un resumen de las particiones existentes.
    *   Validar que el esquema (columnas) sea el correcto.
    *   Verificar que el número total de filas en Bronze coincida con los CSV originales (esta parte es lenta).

---

## 1) Setup y Conexión

```{python}
import os
import sys
import polars as pl
import gcsfs
from tqdm import tqdm

# --- Parámetros ---
GCS_BUCKET = "tesis-vonetto-datalake"
BRONZE_VIAJES_PATH = f"gs://{GCS_BUCKET}/lake/bronze/viajes"
RAW_PATH = f"gs://{GCS_BUCKET}/raw"

# --- Conexión ---
def enable_adc_crossplatform():
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
        return
    if sys.platform.startswith("win"):
        adc_path = os.path.join(os.environ["APPDATA"], "gcloud", "application_default_credentials.json")
    else:
        adc_path = os.path.expanduser("~/.config/gcloud/application_default_credentials.json")
    if not os.path.exists(adc_path):
        raise FileNotFoundError(f"No se encontró ADC en {adc_path}. Ejecuta: gcloud auth application-default login")
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = adc_path

try:
    enable_adc_crossplatform()
    gfs = gcsfs.GCSFileSystem(token="google_default")
    print("✅ Conexión con GCS establecida.")
except FileNotFoundError as e:
    print(f"❌ Error de autenticación: {e}")

```

## 2) Resumen de Particiones en Bronze

Primero, escaneamos la capa Bronze para ver qué particiones (`semana_iso`) existen y cuántos registros tiene cada una. Esto nos da una vista general rápida del estado del Data Lake.

```{python}
def scan_bronze_viajes() -> pl.LazyFrame:
    """Escanea el directorio de viajes en la capa Bronze."""
    try:
        return pl.scan_parquet(f"{BRONZE_VIAJES_PATH}/**/*.parquet")
    except Exception as e:
        print(f"No se pudo escanear la ruta {BRONZE_VIAJES_PATH}. ¿Ejecutaste la ingesta?", file=sys.stderr)
        print(f"Error: {e}", file=sys.stderr)
        return pl.LazyFrame([])

lf_viajes_bronze = scan_bronze_viajes()

if lf_viajes_bronze.columns:
    print("Escaneando registros por semana en la capa Bronze...")
    conteo_semanal = (
        lf_viajes_bronze
        .group_by("semana_iso")
        .agg(pl.len().alias("cantidad_registros"))
        .sort("semana_iso")
        .collect()
    )
    
    print("Resumen de particiones en 'bronze/viajes':")
    with pl.Config(tbl_rows=-1):
        print(conteo_semanal)
else:
    print("No se encontraron datos en la capa Bronze de viajes.")

```

## 3) Validación de Integridad

### 3.1) Validación de Conteo de Filas

Esta es la validación más robusta. Compara el total de filas en la capa Bronze con el total de filas en los CSV originales. 

**Advertencia:** Esta celda es lenta, ya que necesita escanear todos los archivos CSV originales desde la red.

```{python}
def get_csv_row_count(file_path: str, separator: str) -> int:
    """Cuenta las filas de un CSV de forma eficiente en memoria."""
    try:
        # scan_csv es mucho más eficiente en memoria que read_csv
        return pl.scan_csv(file_path, separator=separator, has_header=True).select(pl.len()).collect().item()
    except Exception as e:
        print(f"Error contando filas en {os.path.basename(file_path)}: {e}")
        return 0

if lf_viajes_bronze.columns:
    print("--- Validación de Conteo de Filas ---")
    
    # 1. Contar filas en Parquet (rápido)
    print("Contando filas en la capa Bronze (Parquet)... Esto es rápido.")
    total_parquet_rows = lf_viajes_bronze.select(pl.len()).collect().item()
    print(f"Total de filas en Parquet: {total_parquet_rows:,}")

    # 2. Contar filas en CSVs (lento)
    print("\nContando filas en los CSV originales... Esto puede tardar varios minutos.")
    viajes_glob = "raw_csv/source=drive/ingest_date=2025-10-01/*viajes.csv"
    try:
        csv_files = gfs.glob(f"{RAW_PATH}/{viajes_glob}")
        total_csv_rows = 0
        for file in tqdm(csv_files, desc="Escaneando CSVs"):
            # Asumimos el separador por defecto, se podría detectar si fuera necesario
            total_csv_rows += get_csv_row_count(f"gs://{file}", separator=";")
        
        print(f"Total de filas en CSVs: {total_csv_rows:,}")

        # 3. Comparar
        print("\n--- Resultado ---")
        if total_parquet_rows == total_csv_rows:
            print(f"✅ ¡Éxito! El número de filas coincide ({total_parquet_rows:,}).")
        else:
            print(f"❌ ¡Falla! El número de filas no coincide.")
            print(f"   - Filas en Parquet: {total_parquet_rows:,}")
            print(f"   - Filas en CSV:     {total_csv_rows:,}")
            print(f"   - Diferencia:       {abs(total_parquet_rows - total_csv_rows):,}")

    except Exception as e:
        print(f"Ocurrió un error al intentar listar los archivos CSV: {e}")
else:
    print("No hay datos para validar el conteo de filas.")

```